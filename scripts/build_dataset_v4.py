#!/usr/bin/env -S uv run
# /// script
# requires-python = ">=3.10"
# dependencies = [
#   "duckdb>=1.0.0",
#   "datasets>=2.18.0",
#   "pandas>=2.0.0",
#   "tqdm>=4.65.0",
#   "thefuzz>=0.22.0",
#   "huggingface_hub>=0.22.0",
#   "requests>=2.28.0",
#   "ollama>=0.4.0",
# ]
# ///
"""
Build Deforum Prompt Dataset v4 ‚Äî Ollama-Synthesized Cinematic Prompts

ALL responses are generated by Ollama (qwen3:4b) from diverse source material.
No v2/v3 responses used directly. This addresses the root cause of quality issues
in previous dataset versions.

Sources:
  Stage 1: v1 scene_context narratives (~150-500 rows)
  Stage 2: Creative Writing ShareGPT top visual passages (~100-400 rows)
  Stage 3: Gutenberg Sci-Fi top visual chunks (~50-300 rows)

Usage:
    # Pilot (300 rows, ~90 min)
    uv run scripts/build_dataset_v4.py --dry-run --seed 42

    # Pilot with synthesis (300 rows)
    uv run scripts/build_dataset_v4.py --seed 42 --stage1-limit 150 --stage2-limit 100 --stage3-limit 50

    # Full run (~1,200 rows)
    uv run scripts/build_dataset_v4.py --seed 42 --stage1-limit 500 --stage2-limit 400 --stage3-limit 300
"""

import argparse
import hashlib
import json
import os
import random
import re
import time
from typing import Dict, List, Optional, Tuple

import duckdb
import ollama as ollama_lib
import pandas as pd
import requests
from datasets import Dataset, DatasetDict
from huggingface_hub import HfApi
from thefuzz import fuzz
from tqdm import tqdm


# =============================================================================
# CONFIGURATION
# =============================================================================

HF_TOKEN = os.environ.get("HF_TOKEN")
OLLAMA_BASE_URL = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")

SYSTEM_MESSAGE = "You are a cinematic video prompt generator specializing in the De Forum Art Film aesthetic."

# Ollama synthesis settings (proven on RTX 4090)
OLLAMA_MODEL = "qwen3:4b"
OLLAMA_OPTIONS = {
    "temperature": 0.85,
    "top_p": 0.9,
    "num_predict": 3000,  # generous budget: ~2500 thinking + ~500 response tokens
    "repeat_penalty": 1.3,
}

# Synthesis prompt template
SYNTHESIS_PROMPT = """Source material:
---
{source_text}
---

Rewrite as a cinematic video diffusion prompt (40-80 words).

Requirements:
- Start with camera movement (tracking shot, push-in, crane, pan, etc.)
- Include lighting description (NOT "chiaroscuro" ‚Äî use varied terms)
- End with mood/atmosphere
- NO character names (use "the figure", "a silhouette", "a form")
- NO meta-text ("Certainly", "Here's", "Sure", "I'd be happy")
- NO negative prompts or technical parameters
- Film grain, noir aesthetic

Output ONLY the prompt, nothing else. 40-80 words."""

# Keyword lists for visual/atmospheric scoring
SCORING_KEYWORDS = {
    "visual": [
        "light", "shadow", "glow", "shimmer", "dark", "bright", "haze", "fog",
        "smoke", "dust", "flame", "neon", "silhouette", "reflection", "beam",
        "radiance", "gleam", "flicker", "pulse", "luminescence"
    ],
    "atmospheric": [
        "silence", "whisper", "echo", "wind", "rain", "thunder", "creak",
        "hum", "pulse", "breathe", "stillness", "quiet", "hush", "murmur"
    ],
    "texture": [
        "grain", "rough", "smooth", "cold", "warm", "damp", "velvet", "rust",
        "glass", "metal", "concrete", "fabric", "tactile"
    ],
    "movement": [
        "drift", "float", "crawl", "sweep", "cascade", "ripple", "flicker",
        "sway", "surge", "flow", "glide", "linger"
    ],
    "scifi": [
        "hologram", "viewport", "console", "starfield", "nebula", "reactor",
        "dome", "corridor", "airlock", "hull", "cryo", "terminal", "interface"
    ]
}

# Meta-text patterns to reject (only LLM conversational prefixes, not natural words)
META_TEXT_PATTERNS = [
    r"(?i)^certainly[.,!]",                  # "Certainly," at start
    r"(?i)^here'?s\s+(a|the|your|my)",       # "Here's a/the..." at start
    r"(?i)^sure[.,!]\s",                     # "Sure," at start
    r"(?i)^i'?d be happy\b",                 # "I'd be happy" at start
    r"(?i)\btechnical parameters?\s*:",       # "Technical Parameters:"
    r"(?i)\bnegative prompts?\s*:",           # "Negative Prompt:"
    r"(?i)^\s*\**\s*(?:prompt|output)\s*\d*\s*:\s*\**",  # "**Prompt:**" prefixes
]

# Tier configuration
TIER_WORD_COUNTS = {
    "short": (15, 40),
    "medium": (40, 70),
    "detailed": (70, 110),
}


# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def count_words(text: str) -> int:
    """Count words in text."""
    return len(text.split())


def score_text_for_visual_density(text: str, include_scifi: bool = False) -> float:
    """Score text for visual/atmospheric density."""
    text_lower = text.lower()
    score = 0

    for category, keywords in SCORING_KEYWORDS.items():
        if category == "scifi" and not include_scifi:
            continue
        for keyword in keywords:
            score += text_lower.count(keyword)

    word_count = count_words(text)
    if word_count > 0:
        score = score / (word_count ** 0.5)

    return score


def assign_tier(word_count: int) -> str:
    """Assign tier based on word count."""
    if word_count < 40:
        return "short"
    elif word_count < 70:
        return "medium"
    else:
        return "detailed"


def format_chat_template(instruction: str, response: str) -> str:
    """Format as Qwen3 chat template ‚Äî NO Technical Parameters block."""
    text = f"""<|im_start|>system
{SYSTEM_MESSAGE}<|im_end|>
<|im_start|>user
{instruction}<|im_end|>
<|im_start|>assistant
{response}<|im_end|>"""
    return text


def has_meta_text(text: str) -> bool:
    """Check if text contains meta-text patterns."""
    for pattern in META_TEXT_PATTERNS:
        if re.search(pattern, text):
            return True
    return False


def has_repeated_phrases(text: str, phrase_len: int = 5, max_repeats: int = 2) -> bool:
    """Check if any N-word phrase repeats more than max_repeats times."""
    words = text.lower().split()
    if len(words) < phrase_len:
        return False

    phrases = {}
    for i in range(len(words) - phrase_len + 1):
        phrase = " ".join(words[i:i + phrase_len])
        phrases[phrase] = phrases.get(phrase, 0) + 1
        if phrases[phrase] > max_repeats:
            return True
    return False


def clean_ollama_response(text: str) -> str:
    """Clean Ollama response: strip thinking tokens, quotes, word counts, etc."""
    # Remove <think>...</think> blocks (with opening tag)
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL)

    # Handle think=False mode: thinking goes to content without <think> opening tag
    # but still has </think> closing tag ‚Äî take everything after it
    if "</think>" in text:
        text = text.split("</think>", 1)[1]

    # Remove surrounding quotes
    text = text.strip().strip('"').strip("'").strip()

    # Remove "**Prompt:**" or "Prompt:" prefixes
    text = re.sub(r"^\s*\**\s*(?:Prompt|Output)\s*\d*\s*:?\s*\**\s*", "", text, flags=re.IGNORECASE)

    # Remove trailing word count annotations like "(75 words)" or "‚Äî 72 words"
    text = re.sub(r"\s*[\(\[‚Äî‚Äì-]\s*\d+\s*words?\s*[\)\]]?\s*$", "", text, flags=re.IGNORECASE)

    # Remove leading/trailing whitespace and normalize spaces
    text = re.sub(r"\s+", " ", text).strip()

    return text


# =============================================================================
# OLLAMA SYNTHESIS
# =============================================================================

def ollama_generate(source_text: str, model: str = None, retries: int = 2, debug: bool = False) -> Optional[str]:
    """Generate a cinematic prompt from source text using Ollama Python library.

    Uses ollama.chat() which properly separates thinking tokens from content,
    solving the issue where qwen3's thinking consumed the entire num_predict budget.
    """
    model = model or OLLAMA_MODEL

    prompt = SYNTHESIS_PROMPT.format(source_text=source_text[:500])

    messages = [
        {"role": "system", "content": SYSTEM_MESSAGE},
        {"role": "user", "content": prompt},
    ]

    for attempt in range(retries + 1):
        try:
            resp = ollama_lib.chat(
                model=model,
                messages=messages,
                options=OLLAMA_OPTIONS,
            )

            content = (resp.message.content or "").strip()

            if debug:
                print(f"    [DEBUG] Content ({len(content)} chars): {content[:200]}...")

            cleaned = clean_ollama_response(content)

            if debug:
                print(f"    [DEBUG] Cleaned ({count_words(cleaned)} words): {cleaned[:200]}")

            # Validate word count
            wc = count_words(cleaned)
            if wc < 15:
                if debug:
                    print(f"    [DEBUG] REJECT: too_short ({wc} words)")
                if attempt < retries:
                    time.sleep(1)
                    continue
                print(f"  ‚úó Rejected: too_short ({wc} words)")
                return None

            if wc > 120:
                if debug:
                    print(f"    [DEBUG] REJECT: too_long ({wc} words), truncating")
                words = cleaned.split()
                cleaned = " ".join(words[:90])
                if not cleaned.endswith("."):
                    cleaned += "."

            if has_meta_text(cleaned):
                if debug:
                    for pat in META_TEXT_PATTERNS:
                        if re.search(pat, cleaned):
                            print(f"    [DEBUG] REJECT meta pattern: {pat}")
                if attempt < retries:
                    time.sleep(1)
                    continue
                print(f"  ‚úó Rejected: meta_text in '{cleaned[:80]}...'")
                return None

            return cleaned

        except Exception as e:
            if attempt < retries:
                time.sleep(2)
                continue
            print(f"  ‚ö†Ô∏è Ollama error after {retries + 1} attempts: {e}")
            return None

    return None


def batch_synthesize(
    source_texts: List[str],
    instructions: List[str],
    model: str = None,
    desc: str = "Synthesizing",
    debug: bool = False,
) -> List[Tuple[str, str, str]]:
    """Synthesize prompts from source texts. Returns (instruction, response, source_text) tuples."""
    results = []
    failed = 0

    for source_text, instruction in tqdm(zip(source_texts, instructions), total=len(source_texts), desc=desc):
        response = ollama_generate(source_text, model=model, debug=debug)

        if response is None:
            failed += 1
            continue

        results.append((instruction, response, source_text))

    print(f"  Synthesized: {len(results)}, Failed: {failed}")
    return results


# =============================================================================
# STAGE 1: v1 Scene Contexts
# =============================================================================

def stage1_v1_scenes(rng: random.Random, limit: int = 150, dry_run: bool = False) -> pd.DataFrame:
    """Extract scene_context from v1 dataset and synthesize via Ollama."""
    print("\n" + "=" * 60)
    print("STAGE 1: v1 Scene Contexts")
    print("=" * 60)

    conn = duckdb.connect()
    if HF_TOKEN:
        conn.execute("CREATE SECRET hf_token (TYPE HUGGINGFACE, TOKEN ?);", [HF_TOKEN])

    path = "hf://datasets/Limbicnation/deforum-prompt-lora-dataset@~parquet/default/train/*.parquet"
    print(f"Loading from: {path}")

    try:
        # First check schema
        schema_df = conn.execute(f"DESCRIBE SELECT * FROM read_parquet('{path}') LIMIT 1").fetchdf()
        print(f"  Columns: {list(schema_df['column_name'])}")

        df = conn.execute(f"SELECT * FROM read_parquet('{path}')").fetchdf()
    except Exception as e:
        print(f"Error loading v1 dataset: {e}")
        conn.close()
        return pd.DataFrame()

    conn.close()
    print(f"Loaded {len(df)} rows")

    # Determine which column has the scene narrative
    # v1 might have: instruction, response, scene_context, etc.
    scene_col = None
    for col_name in ["scene_context", "response", "output"]:
        if col_name in df.columns:
            scene_col = col_name
            break

    if scene_col is None:
        print(f"  ‚ö†Ô∏è No scene column found. Available: {list(df.columns)}")
        return pd.DataFrame()

    print(f"  Using column: '{scene_col}'")

    # Get instruction column
    instr_col = None
    for col_name in ["instruction", "input", "prompt"]:
        if col_name in df.columns:
            instr_col = col_name
            break

    # Extract unique scene texts
    scenes = df[scene_col].dropna().astype(str).tolist()
    instructions_raw = df[instr_col].dropna().astype(str).tolist() if instr_col else ["" for _ in scenes]

    # Deduplicate by first 100 chars
    seen = set()
    unique_scenes = []
    unique_instructions = []
    for scene, instr in zip(scenes, instructions_raw):
        key = scene[:100].lower().strip()
        if key in seen or count_words(scene) < 20:
            continue
        seen.add(key)
        unique_scenes.append(scene)
        unique_instructions.append(instr)

    print(f"  Unique scenes (>20 words): {len(unique_scenes)}")

    # Sample
    if len(unique_scenes) > limit:
        indices = rng.sample(range(len(unique_scenes)), limit)
        unique_scenes = [unique_scenes[i] for i in indices]
        unique_instructions = [unique_instructions[i] for i in indices]

    print(f"  Sampled: {len(unique_scenes)}")

    if dry_run:
        # Synthesize just 5 samples
        dry_limit = min(5, len(unique_scenes))
        unique_scenes = unique_scenes[:dry_limit]
        unique_instructions = unique_instructions[:dry_limit]
        print(f"  [DRY RUN] Synthesizing {dry_limit} samples")

    # Build instructions from scene content
    synth_instructions = []
    for scene, orig_instr in zip(unique_scenes, unique_instructions):
        if orig_instr and count_words(orig_instr) > 3:
            # Clean original instruction
            instr = re.sub(r"Scene\s+\d+\s*:", "", orig_instr).strip()
            synth_instructions.append(f"Generate a cinematic video prompt for: {instr[:120]}")
        else:
            # Build from first sentence of scene
            first_sent = scene.split(".")[0].strip()
            synth_instructions.append(f"Generate a cinematic video prompt for: {first_sent[:120]}")

    # Synthesize
    results = batch_synthesize(unique_scenes, synth_instructions, desc="Stage 1 synthesis", debug=dry_run)

    rows = []
    for instruction, response, _ in results:
        word_count = count_words(response)
        tier = assign_tier(word_count)
        text = format_chat_template(instruction, response)

        rows.append({
            "instruction": instruction,
            "response": response,
            "tier": tier,
            "word_count": word_count,
            "text": text,
            "source": "v1_scenes",
        })

    result_df = pd.DataFrame(rows)
    print(f"\nStage 1 Results: {len(result_df)} rows")
    if len(result_df) > 0:
        print(f"  Tier distribution: {result_df['tier'].value_counts().to_dict()}")
        print(f"  Word count: mean={result_df['word_count'].mean():.0f}, "
              f"min={result_df['word_count'].min()}, max={result_df['word_count'].max()}")

    return result_df


# =============================================================================
# STAGE 2: Creative Writing ShareGPT
# =============================================================================

def stage2_creative_writing(rng: random.Random, limit: int = 100, dry_run: bool = False) -> pd.DataFrame:
    """Score Creative Writing passages and synthesize via Ollama."""
    print("\n" + "=" * 60)
    print("STAGE 2: Creative Writing ShareGPT")
    print("=" * 60)

    conn = duckdb.connect()
    if HF_TOKEN:
        conn.execute("CREATE SECRET hf_token (TYPE HUGGINGFACE, TOKEN ?);", [HF_TOKEN])

    path = "hf://datasets/ChaoticNeutrals/Creative_Writing-ShareGPT@~parquet/default/train/*.parquet"
    print(f"Loading from: {path}")

    try:
        df = conn.execute(f"SELECT * FROM read_parquet('{path}')").fetchdf()
    except Exception as e:
        print(f"Error loading Creative Writing dataset: {e}")
        conn.close()
        return pd.DataFrame()

    conn.close()
    print(f"Loaded {len(df)} conversations")

    # Extract and score
    scored_items = []

    for _, row in tqdm(df.iterrows(), total=len(df), desc="Scoring"):
        conversations = row.get("conversations", [])

        if isinstance(conversations, str):
            try:
                conversations = json.loads(conversations)
            except Exception:
                continue

        if conversations is None or len(conversations) < 2:
            continue

        human_msg = None
        gpt_msg = None

        for msg in conversations:
            if isinstance(msg, dict):
                from_field = msg.get("from", "")
                value = msg.get("value", "")

                if from_field in ["human", "user"] and not human_msg:
                    human_msg = value
                elif from_field in ["gpt", "assistant"] and not gpt_msg:
                    gpt_msg = value

                if human_msg and gpt_msg:
                    break

        if not human_msg or not gpt_msg or count_words(gpt_msg) < 30:
            continue

        score = score_text_for_visual_density(gpt_msg)
        scored_items.append({
            "human": human_msg,
            "gpt": gpt_msg,
            "score": score,
        })

    print(f"Extracted {len(scored_items)} valid conversations")

    # Sort and take top 20%
    scored_items.sort(key=lambda x: x["score"], reverse=True)
    top_count = max(int(len(scored_items) * 0.2), 50)
    top_items = scored_items[:top_count]

    # Sample
    if len(top_items) > limit:
        top_items = rng.sample(top_items, limit)

    print(f"Selected {len(top_items)} for synthesis")

    if dry_run:
        dry_limit = min(5, len(top_items))
        top_items = top_items[:dry_limit]
        print(f"  [DRY RUN] Synthesizing {dry_limit} samples")

    # Build synthesis inputs
    source_texts = [item["gpt"] for item in top_items]
    instructions = [
        f"Generate a cinematic video prompt for: {item['human'][:120]}"
        for item in top_items
    ]

    # Synthesize
    results = batch_synthesize(source_texts, instructions, desc="Stage 2 synthesis", debug=dry_run)

    rows = []
    for instruction, response, _ in results:
        word_count = count_words(response)
        tier = assign_tier(word_count)
        text = format_chat_template(instruction, response)

        rows.append({
            "instruction": instruction,
            "response": response,
            "tier": tier,
            "word_count": word_count,
            "text": text,
            "source": "creative_writing",
        })

    result_df = pd.DataFrame(rows)
    print(f"\nStage 2 Results: {len(result_df)} rows")
    if len(result_df) > 0:
        print(f"  Tier distribution: {result_df['tier'].value_counts().to_dict()}")
        print(f"  Word count: mean={result_df['word_count'].mean():.0f}, "
              f"min={result_df['word_count'].min()}, max={result_df['word_count'].max()}")

    return result_df


# =============================================================================
# STAGE 3: Gutenberg Sci-Fi
# =============================================================================

def stage3_gutenberg(rng: random.Random, limit: int = 50, dry_run: bool = False) -> pd.DataFrame:
    """Chunk and score Gutenberg Sci-Fi books, synthesize via Ollama."""
    print("\n" + "=" * 60)
    print("STAGE 3: Gutenberg Sci-Fi")
    print("=" * 60)

    conn = duckdb.connect()
    if HF_TOKEN:
        conn.execute("CREATE SECRET hf_token (TYPE HUGGINGFACE, TOKEN ?);", [HF_TOKEN])

    path = "hf://datasets/stevez80/Sci-Fi-Books-gutenberg@~parquet/default/train/*.parquet"
    print(f"Loading from: {path}")

    try:
        df = conn.execute(f"SELECT * FROM read_parquet('{path}') USING SAMPLE 500").fetchdf()
    except Exception as e:
        print(f"Error loading Gutenberg dataset: {e}")
        conn.close()
        return pd.DataFrame()

    conn.close()
    print(f"Loaded {len(df)} books")

    # Chunk books into ~300-word segments
    chunks = []

    for _, row in tqdm(df.iterrows(), total=len(df), desc="Chunking"):
        text = str(row.get("text", ""))
        title = str(row.get("title", "Unknown"))

        # Strip Gutenberg headers/footers
        text = re.sub(r"\*\*\*\s*START OF.*?\*\*\*", "", text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r"\*\*\*\s*END OF.*?\*\*\*", "", text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r"End of Project Gutenberg.*", "", text, flags=re.DOTALL | re.IGNORECASE)

        words = text.split()
        book_chunks = 0
        i = 0
        while i < len(words) and book_chunks < 5:
            chunk_size = min(350, len(words) - i)
            if chunk_size < 100:
                break

            chunk_words = words[i : i + chunk_size]
            chunk_text = " ".join(chunk_words)

            if 200 <= len(chunk_words) <= 500:
                chunks.append({"text": chunk_text, "title": title})
                book_chunks += 1

            i += chunk_size

    print(f"Created {len(chunks)} chunks")

    # Score chunks
    for chunk in tqdm(chunks, desc="Scoring"):
        chunk["score"] = score_text_for_visual_density(chunk["text"], include_scifi=True)

    # Sort and take top
    chunks.sort(key=lambda x: x["score"], reverse=True)
    top_chunks = chunks[: limit * 3]  # Take extra to account for synthesis failures

    if len(top_chunks) > limit:
        top_chunks = rng.sample(top_chunks, limit)

    print(f"Selected {len(top_chunks)} chunks for synthesis")

    if dry_run:
        dry_limit = min(5, len(top_chunks))
        top_chunks = top_chunks[:dry_limit]
        print(f"  [DRY RUN] Synthesizing {dry_limit} samples")

    # Build synthesis inputs
    source_texts = [chunk["text"] for chunk in top_chunks]
    instructions = []
    for chunk in top_chunks:
        sentences = chunk["text"].split(".")[:2]
        hint = ". ".join(s.strip() for s in sentences if s.strip())
        instructions.append(f"Generate a cinematic video prompt inspired by: {hint[:120]}")

    # Synthesize
    results = batch_synthesize(source_texts, instructions, desc="Stage 3 synthesis", debug=dry_run)

    rows = []
    for instruction, response, _ in results:
        word_count = count_words(response)
        tier = assign_tier(word_count)
        text = format_chat_template(instruction, response)

        rows.append({
            "instruction": instruction,
            "response": response,
            "tier": tier,
            "word_count": word_count,
            "text": text,
            "source": "gutenberg_scifi",
        })

    result_df = pd.DataFrame(rows)
    print(f"\nStage 3 Results: {len(result_df)} rows")
    if len(result_df) > 0:
        print(f"  Tier distribution: {result_df['tier'].value_counts().to_dict()}")
        print(f"  Word count: mean={result_df['word_count'].mean():.0f}, "
              f"min={result_df['word_count'].min()}, max={result_df['word_count'].max()}")

    return result_df


# =============================================================================
# STAGE 4: Quality Filter + Merge + Push
# =============================================================================

def compute_hash(text: str) -> str:
    """Compute hash for blocking in deduplication."""
    return hashlib.md5(text[:40].lower().encode()).hexdigest()[:8]


def fuzzy_deduplicate(df: pd.DataFrame, threshold: int = 85) -> pd.DataFrame:
    """Fuzzy deduplicate responses within tier blocks."""
    print(f"\nDeduplicating {len(df)} rows...")

    df = df.copy()
    df["hash"] = df["response"].apply(compute_hash)

    keep_indices = []

    for (tier, hash_val), group in tqdm(df.groupby(["tier", "hash"]), desc="Deduplicating"):
        if len(group) <= 1:
            keep_indices.extend(group.index.tolist())
            continue

        responses = group["response"].tolist()
        indices = group.index.tolist()

        skip = set()
        for i in range(len(responses)):
            if i in skip:
                continue
            keep_indices.append(indices[i])

            for j in range(i + 1, len(responses)):
                if j in skip:
                    continue
                score = fuzz.token_sort_ratio(responses[i], responses[j])
                if score >= threshold:
                    skip.add(j)

    result = df.loc[keep_indices].drop(columns=["hash"])
    print(f"Deduplicated: {len(df)} ‚Üí {len(result)} rows ({len(df) - len(result)} removed)")

    return result


def quality_filter(df: pd.DataFrame) -> pd.DataFrame:
    """Apply strict quality filters to synthesized responses."""
    print(f"\nQuality filtering {len(df)} rows...")
    original_len = len(df)

    # 1. Word count bounds
    df = df[(df["word_count"] >= 15) & (df["word_count"] <= 110)].copy()
    print(f"  After word count filter (15-110): {len(df)} ({original_len - len(df)} removed)")

    # 2. Meta-text filter
    meta_mask = df["response"].apply(has_meta_text)
    meta_count = meta_mask.sum()
    df = df[~meta_mask]
    print(f"  After meta-text filter: {len(df)} ({meta_count} removed)")

    # 3. Repeated phrase filter
    repeat_mask = df["response"].apply(has_repeated_phrases)
    repeat_count = repeat_mask.sum()
    df = df[~repeat_mask]
    print(f"  After repeated phrase filter: {len(df)} ({repeat_count} removed)")

    # 4. Character name filter (proper nouns that look like character names)
    name_pattern = r"\b(?:Sarah|John|Elena|Mara|Yuki|David|James|Michael|Emma|Alice|Bob)\b"
    name_mask = df["response"].str.contains(name_pattern, flags=re.IGNORECASE, na=False)
    name_count = name_mask.sum()
    df = df[~name_mask]
    print(f"  After character name filter: {len(df)} ({name_count} removed)")

    # 5. Too short (likely failed synthesis)
    short_mask = df["response"].str.len() < 50
    short_count = short_mask.sum()
    df = df[~short_mask]
    print(f"  After min length filter (<50 chars): {len(df)} ({short_count} removed)")

    print(f"\nTotal filtered: {original_len} ‚Üí {len(df)} ({original_len - len(df)} removed)")

    return df.reset_index(drop=True)


def validate_dataset(df: pd.DataFrame) -> Dict[str, any]:
    """Validate the final dataset."""
    print("\n" + "=" * 60)
    print("VALIDATION")
    print("=" * 60)

    checks = {
        "total_rows": len(df),
        "has_instruction": (df["instruction"].str.len() > 0).all(),
        "has_response": (df["response"].str.len() > 0).all(),
        "has_chat_tokens": df["text"].str.contains("<\\|im_start\\|>", regex=True).all(),
        "word_counts_valid": True,
        "sarah_pct": 0.0,
        "chiaroscuro_pct": 0.0,
        "tier_dist": {},
        "source_dist": {},
    }

    # Check word count ranges per tier
    for tier, (min_w, max_w) in TIER_WORD_COUNTS.items():
        tier_df = df[df["tier"] == tier]
        if len(tier_df) > 0:
            invalid = ((tier_df["word_count"] < min_w) | (tier_df["word_count"] > max_w)).sum()
            if invalid > len(tier_df) * 0.15:
                checks["word_counts_valid"] = False
                print(f"  ‚ö†Ô∏è {tier}: {invalid}/{len(tier_df)} rows outside {min_w}-{max_w} word range")

    sarah_count = df["response"].str.contains("Sarah", case=False, na=False).sum()
    checks["sarah_pct"] = 100 * sarah_count / len(df) if len(df) > 0 else 0

    chiaroscuro_count = df["response"].str.contains("chiaroscuro", case=False, na=False).sum()
    checks["chiaroscuro_pct"] = 100 * chiaroscuro_count / len(df) if len(df) > 0 else 0

    # No Technical Parameters in responses
    tech_params_count = df["text"].str.contains("Technical Parameters:", na=False).sum()

    checks["tier_dist"] = df["tier"].value_counts().to_dict()
    checks["source_dist"] = df["source"].value_counts().to_dict()

    print(f"Total rows: {checks['total_rows']}")
    print(f"Has instruction: {'‚úÖ' if checks['has_instruction'] else '‚ùå'}")
    print(f"Has response: {'‚úÖ' if checks['has_response'] else '‚ùå'}")
    print(f"Has chat tokens: {'‚úÖ' if checks['has_chat_tokens'] else '‚ùå'}")
    print(f"Word counts valid: {'‚úÖ' if checks['word_counts_valid'] else '‚ùå'}")
    print(f"Sarah: {sarah_count} ({checks['sarah_pct']:.1f}%) {'‚úÖ' if checks['sarah_pct'] == 0 else '‚ö†Ô∏è'}")
    print(f"Chiaroscuro: {chiaroscuro_count} ({checks['chiaroscuro_pct']:.1f}%) {'‚úÖ' if checks['chiaroscuro_pct'] < 5 else '‚ö†Ô∏è'}")
    print(f"Technical Parameters: {tech_params_count} {'‚úÖ' if tech_params_count == 0 else '‚ùå'}")
    print(f"Tier distribution: {checks['tier_dist']}")
    print(f"Source distribution: {checks['source_dist']}")

    # Sample outputs
    print("\n" + "=" * 60)
    print("SAMPLE OUTPUTS")
    print("=" * 60)

    for source in df["source"].unique():
        source_df = df[df["source"] == source]
        if len(source_df) > 0:
            sample = source_df.sample(1, random_state=42).iloc[0]
            print(f"\n--- {source.upper()} ---")
            print(f"Instruction: {sample['instruction'][:100]}...")
            print(f"Response: {sample['response']}")
            print(f"Word count: {sample['word_count']}, Tier: {sample['tier']}")

    return checks


def stage4_merge_and_push(
    dfs: List[pd.DataFrame],
    target: str,
    dry_run: bool = False,
) -> Optional[DatasetDict]:
    """Quality filter, merge, deduplicate, and push to Hub."""
    print("\n" + "=" * 60)
    print("STAGE 4: Quality Filter + Merge + Push")
    print("=" * 60)

    # Combine all sources
    combined = pd.concat(dfs, ignore_index=True)
    print(f"Combined: {len(combined)} rows")

    # Quality filter (strict)
    filtered = quality_filter(combined)

    # Fuzzy dedup
    deduped = fuzzy_deduplicate(filtered, threshold=85)

    # Re-assign tiers after filtering
    deduped["word_count"] = deduped["response"].apply(count_words)
    deduped["tier"] = deduped["word_count"].apply(assign_tier)

    # Validate
    checks = validate_dataset(deduped)

    all_pass = (
        checks["has_instruction"]
        and checks["has_response"]
        and checks["has_chat_tokens"]
        and checks["sarah_pct"] == 0
        and checks["chiaroscuro_pct"] < 5
        and len(deduped) >= 50  # At least some data
    )

    if not all_pass:
        print("\n‚ö†Ô∏è Validation issues detected ‚Äî review before pushing")

    if dry_run:
        print(f"\n[DRY RUN] Would push to Hub:")
        print(f"  Dataset: {target}")
        print(f"  Rows: {len(deduped)}")
        return None

    # Shuffle and split 90/10
    deduped = deduped.sample(frac=1, random_state=42).reset_index(drop=True)
    split_idx = int(len(deduped) * 0.9)

    train_df = deduped.iloc[:split_idx]
    val_df = deduped.iloc[split_idx:]

    print(f"\nSplit: {len(train_df)} train, {len(val_df)} validation")

    # Create datasets
    columns = ["instruction", "response", "tier", "word_count", "text", "source"]
    train_ds = Dataset.from_pandas(train_df[columns])
    val_ds = Dataset.from_pandas(val_df[columns])

    dataset_dict = DatasetDict({"train": train_ds, "validation": val_ds})

    print(f"\nPushing to HuggingFace Hub: {target}")
    try:
        dataset_dict.push_to_hub(target, token=HF_TOKEN, private=True)
        print(f"‚úÖ Successfully pushed to: https://huggingface.co/datasets/{target}")
    except Exception as e:
        print(f"‚ùå Failed to push: {e}")
        # Save locally as fallback
        local_path = f"./data/{target.split('/')[-1]}"
        os.makedirs(local_path, exist_ok=True)
        deduped.to_json(f"{local_path}/data.jsonl", orient="records", lines=True)
        print(f"  üíæ Saved locally to: {local_path}/data.jsonl")
        return None

    return dataset_dict


# =============================================================================
# MAIN
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Build Deforum Prompt Dataset v4 (Ollama Synthesis)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument("--dry-run", action="store_true", help="Synthesize 5 samples per source, print stats")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--target", type=str, default="Limbicnation/deforum-prompt-lora-dataset-v4",
                        help="Target dataset ID on Hub")
    parser.add_argument("--ollama-model", type=str, default=OLLAMA_MODEL, help="Ollama model for synthesis")
    parser.add_argument("--stage1-limit", type=int, default=150, help="Max rows from v1 scenes")
    parser.add_argument("--stage2-limit", type=int, default=100, help="Max rows from Creative Writing")
    parser.add_argument("--stage3-limit", type=int, default=50, help="Max rows from Gutenberg Sci-Fi")
    parser.add_argument("--skip-stage1", action="store_true", help="Skip Stage 1")
    parser.add_argument("--skip-stage2", action="store_true", help="Skip Stage 2")
    parser.add_argument("--skip-stage3", action="store_true", help="Skip Stage 3")
    args = parser.parse_args()

    model_name = args.ollama_model

    print("=" * 60)
    print("BUILD DATASET v4 (OLLAMA SYNTHESIS)")
    print("=" * 60)
    print(f"Mode: {'DRY RUN' if args.dry_run else 'FULL'}")
    print(f"Seed: {args.seed}")
    print(f"Model: {model_name}")
    print(f"Limits: Stage1={args.stage1_limit}, Stage2={args.stage2_limit}, Stage3={args.stage3_limit}")
    print(f"Target: {args.target}")

    # Verify Ollama is running
    try:
        model_list = ollama_lib.list()
        models = [m.model for m in model_list.models]
        print(f"\nOllama models available: {models}")
        if not any(model_name in m for m in models):
            print(f"‚ö†Ô∏è Model '{model_name}' not found. Available: {models}")
            return
    except Exception as e:
        print(f"‚ùå Cannot connect to Ollama: {e}")
        print("  Make sure Ollama is running: ollama serve")
        return

    rng = random.Random(args.seed)
    start_time = time.time()

    dfs = []

    if not args.skip_stage1:
        df1 = stage1_v1_scenes(rng, limit=args.stage1_limit, dry_run=args.dry_run)
        if len(df1) > 0:
            dfs.append(df1)

    if not args.skip_stage2:
        df2 = stage2_creative_writing(rng, limit=args.stage2_limit, dry_run=args.dry_run)
        if len(df2) > 0:
            dfs.append(df2)

    if not args.skip_stage3:
        df3 = stage3_gutenberg(rng, limit=args.stage3_limit, dry_run=args.dry_run)
        if len(df3) > 0:
            dfs.append(df3)

    if not dfs:
        print("\n‚ùå No data produced from any stage")
        return

    # Stage 4: Merge, filter, push
    result = stage4_merge_and_push(dfs, target=args.target, dry_run=args.dry_run)

    elapsed = time.time() - start_time
    print(f"\n{'=' * 60}")
    print(f"PIPELINE COMPLETE ‚Äî {elapsed / 60:.1f} minutes")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    main()
